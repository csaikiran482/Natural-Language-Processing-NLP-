{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1345f86",
   "metadata": {},
   "source": [
    "# Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5926c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "astring = 'Hello world'\n",
    "print(astring.startswith(\"Hello\"))\n",
    "print(astring.endswith(\"rld\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5d267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "print(astring.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3317ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "print(astring.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d36806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hell', ' w', 'rld']\n"
     ]
    }
   ],
   "source": [
    "print(astring.split('o'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e6af01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He visited Beijing last year .\n"
     ]
    }
   ],
   "source": [
    "string_list = ['He','visited','Beijing','last','year','.']\n",
    "print(' '.join(string_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daf1e112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He*visited*Beijing*last*year*.\n"
     ]
    }
   ],
   "source": [
    "print('*'.join(string_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0b6df72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love Geeks for \"Geeks!\"\n"
     ]
    }
   ],
   "source": [
    "print('I love {} for \"{}!\"'.format('Geeks', 'Geeks')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49db2c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geeks and Portal\n"
     ]
    }
   ],
   "source": [
    "print('{0} and {1}'.format('Geeks', 'Portal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "019d8f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portal and Geeks\n"
     ]
    }
   ],
   "source": [
    "print('{1} and {0}'.format('Geeks', 'Portal')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "957f9efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number one portal is Geeks, For, and Geeks.\n"
     ]
    }
   ],
   "source": [
    "print('Number one portal is {0}, {1}, and {other}.'\n",
    "     .format('Geeks', 'For', other ='Geeks')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef6994d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geeks :12, Portal :    0.55\n"
     ]
    }
   ],
   "source": [
    "print(\"Geeks :{0:2d}, Portal :{1:8.2f}\". \n",
    "      format(12, 00.546)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5c2b970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second argument:  11, first one:   47.42\n"
     ]
    }
   ],
   "source": [
    "print(\"Second argument: {1:3d}, first one: {0:7.2f}\". \n",
    "      format(47.42, 11)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d09f6d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geeks:   453,  Portal:    59.06\n"
     ]
    }
   ],
   "source": [
    "print(\"Geeks: {a:5d},  Portal: {p:8.2f}\". \n",
    "     format(a = 453, p = 59.058))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75848365",
   "metadata": {},
   "source": [
    "# lab1.1-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f41b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install nltk if you're not using virtual environment: \n",
    "# !pip install nltk\n",
    "#nltk.download('punkt')\n",
    "import nltk as nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd0d61f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All work and no play makes jack dull boy.', 'All work and no play makes jack a dull boy.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"All work and no play makes jack dull boy. \\\n",
    "All work and no play makes jack a dull boy.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec202c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'dull', 'boy.', 'All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy.']\n"
     ]
    }
   ],
   "source": [
    "# simplest tokenization: just split words by whitespaces\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5620147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', '', ' work and no p', 'ay makes jack du', '', ' boy. A', '', ' work and no p', 'ay makes jack a du', '', ' boy.']\n"
     ]
    }
   ],
   "source": [
    "print(text.split('l'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65926048",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Sunil tweeted at U.S.A., \"Witnessing 70th \\\n",
    "Republic Day of India from Rajpath, \\\n",
    "New Delhi. Mesmerizing performance by Indian Army! \\\n",
    "Exciting to see the state-of-the-art weapons! \\\n",
    "Awesome airshow! @india_official \\\n",
    "@indian_army #India #70thRepublic_Day. \\\n",
    "For more photos ping me at e-mail email sunil@photoking.com :)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88efbd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted', 'at', 'U.S.A.,', '\"Witnessing', '70th', 'Republic', 'Day', 'of', 'India', 'from', 'Rajpath,', 'New', 'Delhi.', 'Mesmerizing', 'performance', 'by', 'Indian', 'Army!', 'Exciting', 'to', 'see', 'the', 'state-of-the-art', 'weapons!', 'Awesome', 'airshow!', '@india_official', '@indian_army', '#India', '#70thRepublic_Day.', 'For', 'more', 'photos', 'ping', 'me', 'at', 'e-mail', 'email', 'sunil@photoking.com', ':)']\n"
     ]
    }
   ],
   "source": [
    "# simplest tokenization: just split words by whitespaces\n",
    "print(sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca0687",
   "metadata": {},
   "source": [
    "# Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8de22b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted', 'at', 'U.S.A.', ',', '``', 'Witnessing', '70th', 'Republic', 'Day', 'of', 'India', 'from', 'Rajpath', ',', 'New', 'Delhi', '.', 'Mesmerizing', 'performance', 'by', 'Indian', 'Army', '!', 'Exciting', 'to', 'see', 'the', 'state-of-the-art', 'weapons', '!', 'Awesome', 'airshow', '!', '@', 'india_official', '@', 'indian_army', '#', 'India', '#', '70thRepublic_Day', '.', 'For', 'more', 'photos', 'ping', 'me', 'at', 'e-mail', 'email', 'sunil', '@', 'photoking.com', ':', ')']\n"
     ]
    }
   ],
   "source": [
    "# using nltk's tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5582a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk as nlp\n",
    "# sentence = '\"he told the BBC that, 2 people a second were now being given the jab, usually the first dose of two.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2d3a2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted', 'at', 'U.S.A.', ',', '``', 'Witnessing', '70th', 'Republic', 'Day', 'of', 'India', 'from', 'Rajpath', ',', 'New', 'Delhi', '.', 'Mesmerizing', 'performance', 'by', 'Indian', 'Army', '!', 'Exciting', 'to', 'see', 'the', 'state-of-the-art', 'weapons', '!', 'Awesome', 'airshow', '!', '@', 'india_official', '@', 'indian_army', '#', 'India', '#', '70thRepublic_Day', '.', 'For', 'more', 'photos', 'ping', 'me', 'at', 'e-mail', 'email', 'sunil', '@', 'photoking.com', ':', ')']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db62d64",
   "metadata": {},
   "source": [
    "# Types and Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2dc49f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'!': 3, '@': 3, 'at': 2, ',': 2, 'India': 2, '.': 2, '#': 2, 'Sunil': 1, 'tweeted': 1, 'U.S.A.': 1, '``': 1, 'Witnessing': 1, '70th': 1, 'Republic': 1, 'Day': 1, 'of': 1, 'from': 1, 'Rajpath': 1, 'New': 1, 'Delhi': 1, 'Mesmerizing': 1, 'performance': 1, 'by': 1, 'Indian': 1, 'Army': 1, 'Exciting': 1, 'to': 1, 'see': 1, 'the': 1, 'state-of-the-art': 1, 'weapons': 1, 'Awesome': 1, 'airshow': 1, 'india_official': 1, 'indian_army': 1, '70thRepublic_Day': 1, 'For': 1, 'more': 1, 'photos': 1, 'ping': 1, 'me': 1, 'e-mail': 1, 'email': 1, 'sunil': 1, 'photoking.com': 1, ':': 1, ')': 1})\n"
     ]
    }
   ],
   "source": [
    "types = nlp.Counter(words)\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4840749",
   "metadata": {},
   "source": [
    "# Tweet Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c6cd79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted', 'at', 'U', '.', 'S', '.', 'A', '.', ',', '\"', 'Witnessing', '70th', 'Republic', 'Day', 'of', 'India', 'from', 'Rajpath', ',', 'New', 'Delhi', '.', 'Mesmerizing', 'performance', 'by', 'Indian', 'Army', '!', 'Exciting', 'to', 'see', 'the', 'state-of-the-art', 'weapons', '!', 'Awesome', 'airshow', '!', '@india_official', '@indian_army', '#India', '#70thRepublic_Day', '.', 'For', 'more', 'photos', 'ping', 'me', 'at', 'e-mail', 'email', 'sunil@photoking.com', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "print(TweetTokenizer().tokenize(sentence))\n",
    "\n",
    "# Special texts, like Twitter tweets, have a characteristic structure and \n",
    "# the generic tokenizers mentioned above fail to produce viable tokens when \n",
    "# applied to these datasets. NLTK offers a special tokenizer for tweets to help in this case. \n",
    "# This is a rule-based tokenizer that can remove HTML code, remove problematic characters, \n",
    "# remove Twitter handles, and normalize text length by reducing the occurrence of repeated letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3443b",
   "metadata": {},
   "source": [
    "# Text blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ae12fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TextBlob\n",
      "  Using cached textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from TextBlob) (3.6.1)\n",
      "Requirement already satisfied: regex in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob) (1.0.1)\n",
      "Installing collected packages: TextBlob\n",
      "Successfully installed TextBlob-0.15.3\n",
      "['Sunil', 'tweeted', 'at', 'U.S.A', 'Witnessing', '70th', 'Republic', 'Day', 'of', 'India', 'from', 'Rajpath', 'New', 'Delhi', 'Mesmerizing', 'performance', 'by', 'Indian', 'Army', 'Exciting', 'to', 'see', 'the', 'state-of-the-art', 'weapons', 'Awesome', 'airshow', 'india_official', 'indian_army', 'India', '70thRepublic_Day', 'For', 'more', 'photos', 'ping', 'me', 'at', 'e-mail', 'email', 'sunil', 'photoking.com']\n"
     ]
    }
   ],
   "source": [
    "# using textblob's tokenizer\n",
    "!pip install TextBlob\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(sentence)\n",
    "print(blob.words)\n",
    "#We could notice that the TextBlob tokenizer removes the punctuations. \n",
    "#In addition, it has rules for English contractions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb72fa5",
   "metadata": {},
   "source": [
    "# Multi-Word Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a078d2",
   "metadata": {},
   "source": [
    "MWE tokenizer: MWE stands for Multi-Word Expression. Here, certain groups of multiple words are treated as one entity during tokenization, such as \"United States of America,\" \"People's Republic of China,\" \"not only\", etc.\n",
    "\n",
    "Regular expression tokenizer: These tokenizers are developed using regular expressions. Sentences are split based on the occurrence of a particular pattern.\n",
    "\n",
    "Word Punkt tokenizer: This splits a text into a list of alphabetical characters, digits, and non-alphabetical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ffb7827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted', 'at', 'U.S.A.,', '\"Witnessing', '70th', 'Republic_Day', 'of', 'India', 'from', 'Rajpath,', 'New', 'Delhi.', 'Mesmerizing', 'performance', 'by', 'Indian', 'Army!', 'Exciting', 'to', 'see', 'the', 'state-of-the-art', 'weapons!', 'Awesome', 'airshow!', '@india_official', '@indian_army', '#India', '#70thRepublic_Day.', 'For', 'more', 'photos', 'ping', 'me', 'at', 'e-mail', 'email', 'sunil@photoking.com', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "mwe_tokenizer = MWETokenizer([('Republic', 'Day')])\n",
    "mwe_tokenizer.add_mwe(('Indian', 'Army'))\n",
    "print(mwe_tokenizer.tokenize(sentence.split()))\n",
    "# note that 'Indian Army' should be treated as one word, but fails. Why? How to fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0159f",
   "metadata": {},
   "source": [
    "# Word Punkt tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2c2b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted', 'at', 'U', '.', 'S', '.', 'A', '.,', '\"', 'Witnessing', '70th', 'Republic', 'Day', 'of', 'India', 'from', 'Rajpath', ',', 'New', 'Delhi', '.', 'Mesmerizing', 'performance', 'by', 'Indian', 'Army', '!', 'Exciting', 'to', 'see', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'weapons', '!', 'Awesome', 'airshow', '!', '@', 'india_official', '@', 'indian_army', '#', 'India', '#', '70thRepublic_Day', '.', 'For', 'more', 'photos', 'ping', 'me', 'at', 'e', '-', 'mail', 'email', 'sunil', '@', 'photoking', '.', 'com', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "wp_tokenizer = WordPunctTokenizer()\n",
    "print(wp_tokenizer.tokenize(sentence))\n",
    "# again, abbreviations (USA) are not handled appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ffead3",
   "metadata": {},
   "source": [
    "# Regular expression tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1bb2f380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunil', 'tweeted', 'at', 'U', 'S', 'A', 'Witnessing', '70th', 'Republic', 'Day', 'of', 'India', 'from', 'Rajpath', 'New', 'Delhi', 'Mesmerizing', 'performance', 'by', 'Indian', 'Army', 'Exciting', 'to', 'see', 'the', 'state', 'of', 'the', 'art', 'weapons', 'Awesome', 'airshow', 'india_official', 'indian_army', 'India', '70thRepublic_Day', 'For', 'more', 'photos', 'ping', 'me', 'at', 'e', 'mail', 'email', 'sunil', 'photoking', 'com']\n"
     ]
    }
   ],
   "source": [
    "# regex tokenizer\n",
    "# mistakes/undesirable splits: USA, e-mail\n",
    "# pro: you can write your own regex to define the rule to tokenize\n",
    "from nltk import regexp_tokenize\n",
    "pattern = r'''([A-Z]\\.)+\n",
    "| \\w+(-\\w+)*\n",
    "| \\$?\\d+(\\.\\d+)?\\%?\n",
    "| \\.\\.\\.  \n",
    "| [][.,'\"?():-_`]\n",
    "'''\n",
    "print(regexp_tokenize(sentence,r'\\w+'))\n",
    "sentence = 'Sunil tweeted at U.S.A., \"Witnessing 70th \\\n",
    "Republic Day of India from Rajpath, \\\n",
    "New Delhi. Mesmerizing performance by Indian Army! \\\n",
    "Exciting to see the state-of-the-art weapons! \\\n",
    "Awesome airshow! @india_official \\\n",
    "@indian_army #India #70thRepublic_Day. \\\n",
    "For more photos ping me at e-mail email sunil@photoking.com :)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fc207",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e2f4a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you do not use virtual env, install the nltk library\n",
    "# !pip install --user nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b613a4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n",
      "Lancaster Stemmer\n",
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n"
     ]
    }
   ],
   "source": [
    "#create an object of class PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "#proide a word to be stemmed\n",
    "print(\"Porter Stemmer\")\n",
    "print(porter.stem(\"cats\"))\n",
    "print(porter.stem(\"trouble\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "print(porter.stem(\"troubled\"))\n",
    "print(\"Lancaster Stemmer\")\n",
    "print(lancaster.stem(\"cats\"))\n",
    "print(lancaster.stem(\"trouble\"))\n",
    "print(lancaster.stem(\"troubling\"))\n",
    "print(lancaster.stem(\"troubled\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bfa0bdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n",
      "Pythoners           python              python              \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\", \n",
    "             \"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\",\"Pythoners\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "517f271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\n"
     ]
    }
   ],
   "source": [
    "sentence=\"\"\"Pythoners are very intelligent and work very pythonly and \\\n",
    "now they are pythoning their way to success.\"\"\"\n",
    "print(porter.stem(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cd8d78b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "are\n",
      "veri\n",
      "intellig\n",
      "and\n",
      "work\n",
      "veri\n",
      "pythonli\n",
      "and\n",
      "now\n",
      "they\n",
      "are\n",
      "python\n",
      "their\n",
      "way\n",
      "to\n",
      "success\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "for word in word_tokenize(sentence):\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bdcf03",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0dcbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://corenlp.run/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d62091e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in string.punctuation: # remove all punctuations\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d0e34",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d740f18",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-49-b6ab0858ff2b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-49-b6ab0858ff2b>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    pip install -U spacy\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sentence)\n",
    "print([ww for ww in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9131c",
   "metadata": {},
   "source": [
    "# Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "185d0dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eating', 'would', 'nice', 'sanitize', 'hands', 'sanitizer']\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "sentence = \"Before eating, it would be nice to sanitize your hands with a sanitizer\"\n",
    "ps_stemmer = PorterStemmer()\n",
    "words = word_tokenize(sentence)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(sent_words):\n",
    "    return [ww for ww in sent_words \n",
    "            if ww.lower() not in stop_words and ww not in string.punctuation]\n",
    "\n",
    "print(remove_stopwords(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205d22a",
   "metadata": {},
   "source": [
    "# Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "89d00cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example question\n",
    "import re\n",
    "pos_examples = ['abcdefg','abcde','abcd']\n",
    "neg_examples = ['abc']\n",
    "\n",
    "# This function returns True if the pattern meets all requirements, \n",
    "# and returns False otherwise.\n",
    "# You can re-use this function below\n",
    "def check_correctness(pos_list, neg_list, pattern):\n",
    "    all_examples = pos_list + neg_list\n",
    "    found_items = [entry for entry in all_examples if re.search(pattern, entry)]\n",
    "    #print(found_items) # you could uncomment this line for debugging purpose\n",
    "    return found_items == pos_list \n",
    "\n",
    "pattern = r'abc.+'\n",
    "check_correctness(pos_examples, neg_examples, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21229520",
   "metadata": {},
   "source": [
    "# Text Corpora from NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2de017",
   "metadata": {},
   "source": [
    "Gutenburg Book Copus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe269c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gutenburg Book Copus\n",
    "# NLTK includes a small selection of texts from the Project Gutenberg electronic text archive. \n",
    "# Project Gutenberg (PG) is a volunteer effort to digitize and archive cultural works, to \"encourage the creation and distribution of eBooks\". \n",
    "# It was founded in 1971 by American writer Michael S. Hart and is the oldest digital library. \n",
    "# Most of the items in its collection are the full texts of public domain books. \n",
    "# The Project tries to make these as free as possible, in long-lasting, open formats that can \n",
    "# be used on almost any computer. As of 23 June 2018, Project Gutenberg reached \n",
    "# 57,000 items in its collection of free eBooks.\n",
    "\n",
    "# from nltk.corpus import gutenberg\n",
    "# print(gutenberg.fileids())\n",
    "# emma_words = gutenberg.words('austen-emma.txt')\n",
    "# emma_sents = gutenberg.sents('austen-emma.txt')\n",
    "# emma_raw = gutenberg.raw('austen-emma.txt')\n",
    "# print('===RAW===\\n', emma_raw[:300])\n",
    "# print('===WORDS===\\n',emma_words[:30])\n",
    "# print('===SENTS===\\n',emma_sents[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907729f",
   "metadata": {},
   "source": [
    "Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba58b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Brown University Standard Corpus of Present-Day American English (or just Brown Corpus) \n",
    "# was compiled in the 1960s by Henry Kučera and W. Nelson Francis at Brown University as a general \n",
    "# corpus (text collection) in the field of corpus linguistics. \n",
    "# It contains 500 samples of English-language text, totaling roughly one million words, compiled \n",
    "# from works published in the United States in 1961. \n",
    "#It was the first million-word electronic corpus of English.\n",
    "\n",
    "# from nltk.corpus import brown\n",
    "# print(brown.categories()) \n",
    "# news_words = brown.words(categories='news')\n",
    "# news_sents = brown.sents(categories='news')\n",
    "# news_raw = brown.raw(categories='news')\n",
    "# print('===RAW===\\n', news_raw[:300])\n",
    "# print('===WORDS===\\n',news_words[:30])\n",
    "# print('===SENTS===\\n',news_sents[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c592c31",
   "metadata": {},
   "source": [
    "Shakespeare Corpus:\n",
    "This corpus includes the text of eight books written by Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "981c6a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import shakespeare\n",
    "# print(shakespeare.fileids())\n",
    "# # NOTE: this corpus does not support the .sents function as in the previous corpora\n",
    "# dream_words = shakespeare.words('dream.xml')\n",
    "# print(dream_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b74fe",
   "metadata": {},
   "source": [
    "Inaugural Speech Corpus:\n",
    "This corpus includes the script of 58 US presidents' inaugural addresses, from Washington's in \n",
    "1798 to Trump's in 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec4d38",
   "metadata": {},
   "source": [
    "Reuters Corpus: \n",
    "The Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The documents have been classified into 90 topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39fd80",
   "metadata": {},
   "source": [
    "Web and Chat Text\n",
    "This corpus includes content from a Firefox discussion forum, conversations overheard in New York, the movie script of Pirates of the Carribean, personal advertisements, and wine reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089a312",
   "metadata": {},
   "source": [
    "# POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee21831f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: ['can', 'you', 'please', 'buy', 'me', 'an', 'Arizona', 'Ice', 'Tea', '?', 'It', \"'s\", '$', '0.99', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "import nltk \n",
    "tokens = nltk.word_tokenize(\"can you please buy me an Arizona Ice Tea? It's $0.99.\")\n",
    "print(\"Token:\",tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3991d5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [('can', 'MD'), ('you', 'PRP'), ('please', 'VB'), ('buy', 'VB'), ('me', 'PRP'), ('an', 'DT'), ('Arizona', 'NNP'), ('Ice', 'NNP'), ('Tea', 'NNP'), ('?', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('$', '$'), ('0.99', 'CD'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "import nltk \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tokens = nltk.word_tokenize(\"can you please buy me an Arizona Ice Tea? It's $0.99.\")\n",
    "print(\"Token:\",nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78144c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\tpenn\tuniversal\n",
      "can\tMD\tAUX\n",
      "you\tPRP\tPRON\n",
      "please\tUH\tINTJ\n",
      "buy\tVB\tVERB\n",
      "me\tPRP\tPRON\n",
      "an\tDT\tDET\n",
      "Arizona\tNNP\tPROPN\n",
      "Ice\tNNP\tPROPN\n",
      "Tea\tNNP\tPROPN\n",
      "?\t.\tPUNCT\n",
      "It\tPRP\tPRON\n",
      "'s\tVBZ\tAUX\n",
      "$\t$\tSYM\n",
      "0.99\tCD\tNUM\n",
      ".\t.\tPUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "print('word\\tpenn\\tuniversal')\n",
    "doc = nlp(\"can you please buy me an Arizona Ice Tea? It's $0.99.\")\n",
    "for ww in doc:\n",
    "    print('{}\\t{}\\t{}'.format(ww,ww.tag_,ww.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa265d",
   "metadata": {},
   "source": [
    "# POS : Ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "529d029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They\tPRP\tPRON\n",
      "refuse\tVBP\tVERB\n",
      "to\tTO\tPART\n",
      "permit\tVB\tVERB\n",
      "us\tPRP\tPRON\n",
      "to\tTO\tPART\n",
      "obtain\tVB\tVERB\n",
      "the\tDT\tDET\n",
      "refuse\tNN\tNOUN\n",
      "permit\tNN\tNOUN\n",
      ".\t.\tPUNCT\n"
     ]
    }
   ],
   "source": [
    "# Definition: “Homonyms”\n",
    "# Two distinct words having the same spelling are called homonyms.\n",
    "# \"refuse\" and \"permit\" having Verb,NN\n",
    "doc = nlp(\"They refuse to permit us to obtain the refuse permit.\")\n",
    "for ww in doc:\n",
    "    print('{}\\t{}\\t{}'.format(ww,ww.tag_,ww.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b6d49",
   "metadata": {},
   "source": [
    "# POS: Extracting Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd17fd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\tPostag\n",
      "He\tPRP\n",
      "went\tVBD\n",
      "to\tIN\n",
      "south\tNNP\n",
      "Africa\tNNP\n",
      "for\tIN\n",
      "Holiday\tNNP\n",
      ".\t.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "doc = nlp(\"He went to south Africa for Holiday.\")\n",
    "print('word\\tPostag')\n",
    "for ww in doc:\n",
    "    print('{}\\t{}'.format(ww,ww.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b643d24",
   "metadata": {},
   "source": [
    "# Syntactic Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b5da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70bb91e0",
   "metadata": {},
   "source": [
    "# Constituency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f74791a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://demo.allennlp.org/constituency-parsing\n",
    "# or \n",
    "#https://corenlp.run/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ce203",
   "metadata": {},
   "source": [
    "# Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09c3106a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"79f7f38b1f6e405b8bc01eff990c745e-0\" class=\"displacy\" width=\"1275\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">shot</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">elephant</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">my</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">pyjamas</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-79f7f38b1f6e405b8bc01eff990c745e-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-79f7f38b1f6e405b8bc01eff990c745e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-79f7f38b1f6e405b8bc01eff990c745e-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-79f7f38b1f6e405b8bc01eff990c745e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-79f7f38b1f6e405b8bc01eff990c745e-0-2\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-79f7f38b1f6e405b8bc01eff990c745e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,266.5 L578.0,254.5 562.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-79f7f38b1f6e405b8bc01eff990c745e-0-3\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-79f7f38b1f6e405b8bc01eff990c745e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,266.5 L758.0,254.5 742.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-79f7f38b1f6e405b8bc01eff990c745e-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-79f7f38b1f6e405b8bc01eff990c745e-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-79f7f38b1f6e405b8bc01eff990c745e-0-5\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-79f7f38b1f6e405b8bc01eff990c745e-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://demo.allennlp.org/dependency-parsing\n",
    "# or\n",
    "# https://corenlp.run/\n",
    "\n",
    "#or\n",
    "from spacy import displacy \n",
    "doc = nlp('I shot an elephant in my pyjamas')\n",
    "displacy.render(doc,style='dep')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2cdae814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I nsubj shot VERB []\n",
      "shot ROOT shot VERB [I, elephant, in]\n",
      "an det elephant NOUN []\n",
      "elephant dobj shot VERB [an]\n",
      "in prep shot VERB [pajamas]\n",
      "my poss pajamas NOUN []\n",
      "pajamas pobj in ADP [my]\n"
     ]
    }
   ],
   "source": [
    "# dependency parsing \n",
    "doc = nlp('I shot an elephant in my pajamas')\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab66b6",
   "metadata": {},
   "source": [
    "# Chuncking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a55da5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n",
      "South Africa\n",
      "Christmas day\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp('He went to South Africa on Christmas day.')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a46a67",
   "metadata": {},
   "source": [
    "# Named entity recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175fae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NE             Type Examples\n",
    "\n",
    "# ORGANIZATION   Georgia-Pacific Corp., WHO\n",
    "# PERSON         Eddy Bonte, President Obama\n",
    "# LOCATION       Murray River, Mount Everest\n",
    "# DATE           June, 2008-06-29\n",
    "# TIME           two fifty a m, 1:30 p.m.\n",
    "# MONEY          175 million Canadian Dollars, GBP 10.40\n",
    "# PERCENT        twenty pct, 18.75 %\n",
    "# FACILITY       Washington Monument, Stonehenge\n",
    "# GPE            South East Asia, Midlothian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "91c38928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named entity 0: Sai Kiran, label PERSON\n",
      "named entity 1: South Africa, label GPE\n",
      "named entity 2: Christmas day, label DATE\n",
      "named entity 3: 12-01-1991, label DATE\n",
      "named entity 4: 200, label CARDINAL\n"
     ]
    }
   ],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp('Sai Kiran went to South Africa on Christmas day in 12-01-1991 nd earned 200.')\n",
    "for i,ent in enumerate(doc.ents):\n",
    "    print('named entity {}: {}, label {}'.format(i,ent.text,ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "36838059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sai Kiran\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " went to \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    South Africa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Christmas day\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    12-01-1991\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " nd earned \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    200\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization of NER results\n",
    "from spacy import displacy\n",
    "displacy.render(doc,style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e5a8a9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon|Amazon|nsubj|purchases\n",
      "Whole Foods|Foods|dobj|purchases\n"
     ]
    }
   ],
   "source": [
    "# chunks also includes dependency information\n",
    "doc = nlp('Amazon purchases Whole Foods for $13.4 billion.')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print('{}|{}|{}|{}'.format(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d1b2bb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon bought Whole Foods\n"
     ]
    }
   ],
   "source": [
    "# A simple application of chunking and dependency parsing\n",
    "def who_purchases_whom(doc):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if 'purchase' in chunk.root.head.text and 'subj' in chunk.root.dep_:\n",
    "            subj = chunk.text\n",
    "        elif 'purchase' in chunk.root.head.text and 'obj' in chunk.root.dep_:\n",
    "            obj = chunk.text\n",
    "    return subj, obj\n",
    "\n",
    "subj, obj = who_purchases_whom(doc)\n",
    "print('{} bought {}'.format(subj,obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033bc675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3af902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5df268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
